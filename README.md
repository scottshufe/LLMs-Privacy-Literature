# LLMs Privacy Literature
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)

A curated list of LLMs Privacy papers (and codes).

Paper are sorted by their released dates in descending order.

## How to Search?
Search keywords like conference name (e.g., ```CCS```), LLMs name (e.g., ```ChatGPT```), or topic (e.g., ```MIA```) over the webpage to quickly locate related papers.

## Quick Links


## LLMs Privacy-related Papers[[Back to Top](#llms-privacy-literature)]

| Year   | Venue | Title | Topic | LLMs  | Paper Link  | Code Link |
|-------|--------|--------|--------|-----------|------------|------------|
| 2023 | NIPS |**DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models** |  |  | [Link](https://arxiv.org/abs//2306.11698) | [Link](https://github.com/AI-secure/DecodingTrust) |
| 2024 | ICML |**Position: TrustLLM: Trustworthiness in Large Language Models** |  |  | [Link](https://proceedings.mlr.press/v235/huang24x.html) | [Link](https://github.com/HowieHwong/TrustLLM) |
| 2024 | ICLR |**Detecting Pretraining Data From Large Language Models** | MIA | LLaMA, GPT-Neo, Pythia | [Link](https://openreview.net/forum?id=zWqr3MQuNs) | [Link](https://github.com/swj0419/detect-pretrain-code) |
| 2024 | EMNLP |**ReCaLL: Membership Inference via Relative Conditional Log-Likelihoods** | MIA | Pythia, GPT-NeoX, LLaMA, OPT, Mamba | [Link](https://arxiv.org/abs/2406.15968) | [Link](https://github.com/ruoyuxie/recall) |
| 2024 | EMNLP |**Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models** | Conversations Reconstruction | GPT-3.5, GPT-4 | [Link](https://arxiv.org/abs/2402.02987) |  |
| 2024 | CCS | **PLeak: Prompt Leaking Attacks against Large Language Model Applications** | Prompt Leaking Attack |  | [Link](https://arxiv.org/abs/2405.06823) | [Link](https://github.com/BHui97/PLeak) |
| 2024 | arXiv | **Evaluating Large Language Model based Personal Information Extraction and Countermeasures** | Personal Profile Extraction |  | [Link](https://arxiv.org/abs/2408.07291) | [Link](https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction) |
| 2024 | arXiv |**Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models** | MIA | Pythia, GPT-NeoX, LLaMA, OPT, Mamba | [Link](https://arxiv.org/abs/2404.02936) | [Link](https://github.com/zjysteven/mink-plus-plus) |



